{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_dqn.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbl1jzu5taY5A1TRYTeEFg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/attoman/ACSL/blob/master/RL_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fdmxYNYP09h"
      },
      "source": [
        "import math, random\r\n",
        "\r\n",
        "import gym\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.autograd as autograd \r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "!pip install gym pyvirtualdisplay\r\n",
        "!apt-get install -y xvfb python-opengl ffmpeg\r\n",
        "\r\n",
        "!apt-get update\r\n",
        "!apt-get install cmake\r\n",
        "!pip install --upgrade setuptools\r\n",
        "!pip install ez_setup\r\n",
        "\r\n",
        "!pip install box2d-py\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO0eussYQSOi"
      },
      "source": [
        "from IPython.display import clear_output\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCaA81rClGZJ"
      },
      "source": [
        "env.render() 함수 결과를 mp4 동영상으로 보여주기 위한 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8r4sgVplJzM"
      },
      "source": [
        "#   env.render() 함수 결과를 mp4 동영상으로 보여주기 위한 코드\r\n",
        "from gym import logger as gymlogger\r\n",
        "from gym.wrappers import Monitor\r\n",
        "gymlogger.set_level(40)     #   error only\r\n",
        "import glob\r\n",
        "import io\r\n",
        "import base64\r\n",
        "from IPython.display import HTML\r\n",
        "from IPython import display as ipythondisplay\r\n",
        "\r\n",
        "\"\"\" Utility functions to enable video recording of gym environment and displaying it To enable video, just do \"env = wrap_env(env)\"\" \"\"\"\r\n",
        "\r\n",
        "def show_video():\r\n",
        "    mp4list = glob.glob('video/*.mp4')\r\n",
        "    if len(mp4list) > 0:\r\n",
        "        mp4 = mp4list[0]\r\n",
        "        video = io.open(mp4, 'r+b').read()\r\n",
        "        encoded = base64.b64encode(video)\r\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay loop controls style=\"height: 400px;\"> <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /> </video>'''.format(encoded.decode('ascii'))))\r\n",
        "    else:\r\n",
        "        print(\"Could not find video\")\r\n",
        "\r\n",
        "def wrap_env(env):\r\n",
        "    env = Monitor(env, './video', force=True)\r\n",
        "    return env\r\n",
        "\r\n",
        "from pyvirtualdisplay import Display\r\n",
        "display = Display(visible=0, size=(1400, 900))\r\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhwJy4X1akv1"
      },
      "source": [
        "Colab 재요청 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c1C2nAdbZ9E"
      },
      "source": [
        "DELAY = INTERVAL = 4 * 60 # interval time in seconds\r\n",
        "MIN_DELAY = MIN_INTERVAL = 2* 60\r\n",
        "KEEPALIVE_URL = \"https://nebula.udacity.com/api/vl/remote/keep-alive\"\r\n",
        "TOKEN_URL = \"http://metadata.google.internal/compuMetadata/vl/instance/attributes/keep_alive_token\"\r\n",
        "TOKEN_HEADERS = {\"Metadata-Flavor\" : \"Google\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfHtHD-Tj2As"
      },
      "source": [
        "def _request_handler(headers) :\r\n",
        "    def _handler(signum, frame) :\r\n",
        "        requests.request(\"POST\", KEEPALIVE_URL, headers=headers)\r\n",
        "    return _handler\r\n",
        "\r\n",
        "@contextmanager\r\n",
        "def active_session(delay=DELAY, interval = INTERVAL) :\r\n",
        "    \"\"\"\r\n",
        "    Example :\r\n",
        "    \r\n",
        "    from workspace_utils import active_session\r\n",
        "\r\n",
        "    with active_sesstion() :\r\n",
        "        # do long-running work here\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    token = requests.request(\"GET\", TOKEN_URL, headers = TOKEN_HEADERS).text\r\n",
        "    headers = {'Authorization' : \"STAR \" + token}\r\n",
        "    delay = max(delay, MIN_DELAY)\r\n",
        "    interval = max(interval, MIN_INTERVAL)\r\n",
        "    original_handler = signal.getsignal(signal.SIGALRM)\r\n",
        "    try :\r\n",
        "        signal.signal(signal.SIGALRM, _request_handler(headers))\r\n",
        "        signal.setitimer(signal.ITIMER_REAL, delay, interval)\r\n",
        "        yield\r\n",
        "    finally :\r\n",
        "        signal.signal(signal.SIGALRM, original_handler)\r\n",
        "        signal.setitimer(signal.ITIMER_REAL, 0)\r\n",
        "\r\n",
        "    def keep_awake(iterable, delay = DELAY, interval = INTERVAL) :\r\n",
        "        \"\"\"\r\n",
        "        Example :\r\n",
        "\r\n",
        "        from workspace_utils import keep_awake\r\n",
        "\r\n",
        "        for i in keep_awake(range(5)) :\r\n",
        "            # do iteration with lots of work here\r\n",
        "        \"\"\"\r\n",
        "        with active_session(delay, interval) : yield from iterable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnumqeHdfTqb"
      },
      "source": [
        "Use CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OomplwHmQcMA"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\r\n",
        "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfF8lRnZfXfn"
      },
      "source": [
        "Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lImjml1UQupO"
      },
      "source": [
        "from collections import deque\r\n",
        "\r\n",
        "class ReplayBuffer(object):\r\n",
        "    def __init__(self, capacity):\r\n",
        "        self.buffer = deque(maxlen=capacity)\r\n",
        "    \r\n",
        "    def push(self, state, action, reward, next_state, done):\r\n",
        "        state      = np.expand_dims(state, 0)\r\n",
        "        next_state = np.expand_dims(next_state, 0)\r\n",
        "            \r\n",
        "        self.buffer.append((state, action, reward, next_state, done))\r\n",
        "    \r\n",
        "    def sample(self, batch_size):\r\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\r\n",
        "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYPw5i00faC5"
      },
      "source": [
        "Cart Pole Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okmi9sEIX2lD"
      },
      "source": [
        "env_id = \"CartPole-v0\"\r\n",
        "env = gym.make(env_id)\r\n",
        "env = wrap_env(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDQgfC7S9ceO"
      },
      "source": [
        "Epsilon greedy exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_xE_q5dYJM2"
      },
      "source": [
        "epsilon_start = 1.0\r\n",
        "epsilon_final = 0.01\r\n",
        "epsilon_decay = 500\r\n",
        "\r\n",
        "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fGKH107Yg7z"
      },
      "source": [
        "plt.plot([epsilon_by_frame(i) for i in range(10000)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCtZJfr18pBm"
      },
      "source": [
        "Deep Q Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKotx68SZhQg"
      },
      "source": [
        "class DQN(nn.Module):\r\n",
        "    def __init__(self, num_inputs, num_actions):\r\n",
        "        super(DQN, self).__init__()\r\n",
        "        \r\n",
        "        self.layers = nn.Sequential(\r\n",
        "            nn.Linear(env.observation_space.shape[0], 128),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(128, 128),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(128, env.action_space.n)\r\n",
        "        )\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        return self.layers(x)\r\n",
        "    \r\n",
        "    def act(self, state, epsilon):\r\n",
        "        if random.random() > epsilon:\r\n",
        "            with torch.no_grad():\r\n",
        "                state = Variable(torch.FloatTensor(state).unsqueeze(0))\r\n",
        "            q_value = self.forward(state)\r\n",
        "            action  = q_value.max(1)[1].data[0].item()\r\n",
        "        else:\r\n",
        "            action = random.randrange(env.action_space.n)\r\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggm4K30TfOIc"
      },
      "source": [
        "model = DQN(env.observation_space.shape[0], env.action_space.n)\r\n",
        "\r\n",
        "if USE_CUDA:\r\n",
        "    model = model.cuda()\r\n",
        "    \r\n",
        "optimizer = optim.Adam(model.parameters())\r\n",
        "\r\n",
        "replay_buffer = ReplayBuffer(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DRYIqSs9g6N"
      },
      "source": [
        "Computing Temporal Difference Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8wOWJz89jt2"
      },
      "source": [
        "def compute_td_loss(batch_size):\r\n",
        "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\r\n",
        "\r\n",
        "    state      = Variable(torch.FloatTensor(np.float32(state)))\r\n",
        "    with torch.no_grad():\r\n",
        "        next_state = Variable(torch.FloatTensor(np.float32(next_state)))\r\n",
        "    action     = Variable(torch.LongTensor(action))\r\n",
        "    reward     = Variable(torch.FloatTensor(reward))\r\n",
        "    done       = Variable(torch.FloatTensor(done))\r\n",
        "\r\n",
        "    q_values      = model(state)\r\n",
        "    next_q_values = model(next_state)\r\n",
        "\r\n",
        "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\r\n",
        "    next_q_value     = next_q_values.max(1)[0]\r\n",
        "    expected_q_value = reward + gamma * next_q_value * (1 - done)\r\n",
        "    \r\n",
        "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\r\n",
        "        \r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "    \r\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFtU75tFC1HC"
      },
      "source": [
        "def plot(frame_idx, rewards, losses):\r\n",
        "    clear_output(True)\r\n",
        "    plt.figure(figsize=(20,5))\r\n",
        "    plt.subplot(131)\r\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\r\n",
        "    plt.plot(rewards)\r\n",
        "    plt.subplot(132)\r\n",
        "    plt.title('loss')\r\n",
        "    plt.plot(losses)\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv4ZFlsLJpyI"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anOjaao4JufS"
      },
      "source": [
        "num_frames = 10000\r\n",
        "batch_size = 32\r\n",
        "gamma      = 0.99\r\n",
        "\r\n",
        "losses = []\r\n",
        "all_rewards = []\r\n",
        "episode_reward = 0\r\n",
        "\r\n",
        "state = env.reset()\r\n",
        "for frame_idx in range(1, num_frames + 1):\r\n",
        "    epsilon = epsilon_by_frame(frame_idx)\r\n",
        "    action = model.act(state, epsilon)\r\n",
        "    # print(action)\r\n",
        "    # print(type(action))\r\n",
        "    next_state, reward, done, _ = env.step(action)\r\n",
        "    replay_buffer.push(state, action, reward, next_state, done)\r\n",
        "    \r\n",
        "    state = next_state\r\n",
        "    episode_reward += reward\r\n",
        "    \r\n",
        "    if done:\r\n",
        "        state = env.reset()\r\n",
        "        all_rewards.append(episode_reward)\r\n",
        "        episode_reward = 0\r\n",
        "        \r\n",
        "    if len(replay_buffer) > batch_size:\r\n",
        "        loss = compute_td_loss(batch_size)\r\n",
        "        losses.append(loss.data)\r\n",
        "        \r\n",
        "    if frame_idx % 200 == 0:\r\n",
        "        plot(frame_idx, all_rewards, losses)\r\n",
        "env.close()\r\n",
        "#show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mmFzCqOr8Tp"
      },
      "source": [
        "Atari Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7UDhsqNsAT3"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount = True)\r\n",
        "import sys\r\n",
        "sys.path.insert(0, '/content/gdrive/MyDrive/Database/')\r\n",
        "from wrappers import make_atari, wrap_deepmind, wrap_pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qqPYTYVsKFO"
      },
      "source": [
        "env_id = \"PongNoFrameskip-v4\"\r\n",
        "env = make_atari(env_id)\r\n",
        "env = wrap_deepmind(env)\r\n",
        "env = wrap_pytorch(env)\r\n",
        "env = wrap_env(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlx1IfXyskNg"
      },
      "source": [
        "class CnnDQN(nn.Module):\r\n",
        "    def __init__(self, input_shape, num_actions):\r\n",
        "        super(CnnDQN, self).__init__()\r\n",
        "\r\n",
        "        self.input_shape = input_shape\r\n",
        "        self.num_actions = num_actions\r\n",
        "\r\n",
        "        self.features = nn.Sequential(\r\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "            nn.Linear(self.feature_size(), 512),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(512, self.num_actions)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.features(x)\r\n",
        "        x = x.view(x.size(0), -1)\r\n",
        "        x = self.fc(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def feature_size(self):\r\n",
        "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1,-1).size(1)\r\n",
        "    \r\n",
        "    def act(self, state, epsilon):\r\n",
        "        if random.random() > epsilon:\r\n",
        "            with torch.no_grad():\r\n",
        "                state = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0))\r\n",
        "            q_value = self.forward(state)\r\n",
        "            action = q_value.max(1)[1].data[0].item()\r\n",
        "        else:\r\n",
        "            action = random.randrange(env.action_space.n)\r\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9qfRcXj0nAw"
      },
      "source": [
        "model = CnnDQN(env.observation_space.shape, env.action_space.n)\r\n",
        "\r\n",
        "if USE_CUDA:\r\n",
        "    model = model.cuda()\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)\r\n",
        "\r\n",
        "replay_initial = 10000\r\n",
        "replay_buffer = ReplayBuffer(100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpZDvN08EkuU"
      },
      "source": [
        "epsilon_start = 1.0\r\n",
        "epsilon_final = 0.01\r\n",
        "epsilon_decay = 30000\r\n",
        "\r\n",
        "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSCygmLfFN3M"
      },
      "source": [
        "plt.plot([epsilon_by_frame(i) for i in range(1000000)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDUd8wZZFVBo"
      },
      "source": [
        "num_frames = 1400000\r\n",
        "batch_size = 32\r\n",
        "gamma = 0.99\r\n",
        "\r\n",
        "losses = []\r\n",
        "all_rewards = []\r\n",
        "episode_reward = 0\r\n",
        "\r\n",
        "state = env.reset()\r\n",
        "for frame_idx in range(1, num_frames + 1):\r\n",
        "    epsilon = epsilon_by_frame(frame_idx)\r\n",
        "    action = model.act(state, epsilon)\r\n",
        "\r\n",
        "    next_state, reward, done, _ = env.step(action)\r\n",
        "    replay_buffer.push(state, action, reward, next_state, done)\r\n",
        "\r\n",
        "    state = next_state\r\n",
        "    episode_reward += reward\r\n",
        "\r\n",
        "    if done:\r\n",
        "        state = env.reset()\r\n",
        "        all_rewards.append(episode_reward)\r\n",
        "        episode_reward = 0\r\n",
        "    \r\n",
        "    if len(replay_buffer) > replay_initial:\r\n",
        "        loss = compute_td_loss(batch_size)\r\n",
        "        losses.append(loss.data)\r\n",
        "\r\n",
        "    if frame_idx % 1000 == 0:\r\n",
        "        plot(frame_idx, all_rewards, losses)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}